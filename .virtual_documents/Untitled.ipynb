











#Importing the necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
#Loading the data
values = pd.read_csv("TrainingsetValues.csv")
labels = pd.read_csv("Trainingsetlabels.csv")
test_values = pd.read_csv("Testsetvalues.csv")


#exploring the datasets
print (values.head())
print(type(values))


print (labels.head())
print(type(labels))


print (test_values.head())
print(type(test_values))


test_values.describe





#Merging values and labels on ID
data = pd.merge(values, labels.copy(), on='id',how ='inner')


data.head()
print(type(data))


data.info


print (data.describe)


print(data.isna().sum())


print(data['status_group'].value_counts())


missing_v = data.isna().mean()*100
missing_v





#The scheme_name column has close to half of its values missing. it has to be dropped.
data =data.drop(["scheme_name","id","date_recorded"], axis =1)
data.head()


from sklearn.impute import SimpleImputer
imputed_data = data.copy()
cat_imputer = SimpleImputer(strategy='most_frequent')
imputed_data[['permit', 'scheme_management', 'public_meeting', 
              'subvillage', 'funder', 'installer', 'wpt_name']] = \
    cat_imputer.fit_transform(imputed_data[['permit', 'scheme_management', 'public_meeting', 
                                             'subvillage', 'funder', 'installer', 'wpt_name']])
print(imputed_data.isna().sum())


for col in imputed_data.select_dtypes(include=[np.number]).columns:
    if col != 'status_group' and col != 'id':
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='status_group', y=col, data=imputed_data)
        plt.title(f'Distribution of {col} by status_group')
        plt.show()





from sklearn.preprocessing import LabelEncoder

# Label encoding ordinal columns
label_encode_cols = ['quality_group', 'quantity_group', 'status_group', 'water_quality', 'quantity']
le_dict = {}
for col in label_encode_cols:
    le = LabelEncoder()
    imputed_data[col] = le.fit_transform(imputed_data[col].astype(str))
    le_dict[col] = le

# One-hot encoding nominal columns using pd.get_dummies
onehot_encode_cols = ['source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']
t_data = pd.get_dummies(imputed_data, columns=onehot_encode_cols, drop_first=True,  dtype=int)
print(t_data.head())






import statsmodels.api as sm
numeric_df = imputed_data.select_dtypes(include=[np.number])
y = imputed_data["status_group"]
X = numeric_df.drop(columns=['status_group'], errors='ignore')
model = sm.OLS(y, sm.add_constant(X))
results = model.fit()
print (f"rsquared = {results.rsquared}")
print (f"rsquared_adjusted = {results.rsquared_adj}")
print (f"fvalue = {results.fvalue}", f"pvalue = {results.f_pvalue}")
print(results.params)


fig = plt.figure(figsize = (12,10))
sm.graphics.plot_partregress_grid(results, fig = fig)
plt.subplots_adjust(wspace=0.4, hspace=0.4) 
plt.tight_layout()
plt.show()








fitted = results.fittedvalues
residuals = results.resid

plt.figure(figsize=(8, 6))
sns.residplot(x=fitted, y=residuals, lowess=True,
              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Fitted Values")
plt.axhline(0, color='black', linestyle='--', lw=1)
plt.show()








from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(C=1.0, solver='liblinear', max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)


#preparing the test data so as to make predictions and evaluate the model
test_data = test_values.copy()
t_data =test_data.drop(["scheme_name","id","date_recorded"], axis =1)
cat_imputer = SimpleImputer(strategy='most_frequent')
test_data[['permit', 'scheme_management', 'public_meeting', 
              'subvillage', 'funder', 'installer', 'wpt_name']] = \
    cat_imputer.fit_transform(test_data[['permit', 'scheme_management', 'public_meeting', 
                                             'subvillage', 'funder', 'installer', 'wpt_name']])
print(test_data.isna().sum())
print(test_data.head())


# Label encode ordinal columns
label_encode_cols = ['quality_group', 'quantity_group',  'water_quality', 'quantity']
le_dict = {}
for col in label_encode_cols:
    le = LabelEncoder()
    test_data[col] = le.fit_transform(test_data[col].astype(str))
    le_dict[col] = le

# One-hot encode nominal columns using pd.get_dummies
onehot_encode_cols = ['source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']
act_data = pd.get_dummies(test_data, columns=onehot_encode_cols, drop_first=True,  dtype=int)
print(act_data.head())











for col in numeric_df.columns:
    col_data = t_data_no_id[col]
    col_skew = skew(col_data)   
    if col_skew > 0:
        if col_data.min() >= 0:
            t_data_no_id['log_' + col] = np.log(col_data + 1)  
        else:
            print(f"Skipping column {col} for positive skew transform because it contains negative values.")
    
    elif col_skew < 0:
        if col_data.max() > 0:
            max_val = col_data.max()
            t_data_no_id['log_' + col] = np.log(max_val - col_data + 1)
        else:
            print(f"Skipping column {col} for negative skew transform because it contains non-positive values.")






original_skew = numeric_df.apply(skew)

transformed_skew = {}
for col in numeric_df.columns:
    log_col = "log_" + col
    if log_col in t_data_no_id.columns:
        transformed_skew[col] = skew(t_data_no_id[log_col])

transformed_skew = pd.Series(transformed_skew)

plt.figure(figsize=(12,6))
original_skew.sort_values().plot(kind='bar', color='blue', alpha=0.7, label='Original')
plt.title('Original Skewness')
plt.legend()
plt.show()

plt.figure(figsize=(12,6))
transformed_skew.sort_values().plot(kind='bar', color='green', alpha=0.7, label='Transformed')
plt.title('Transformed Skewness')
plt.legend()
plt.show()





skewness_comparison = {"Feature": [], "Log Skewness": [], "Sqrt Skewness": []}

for col in t_data_no_id.columns:
    log_col = "log_" + col
    sqrt_col = "sqrt_" + col  

    if log_col in t_data_no_id.columns:
        t_data_no_id[sqrt_col] = np.sqrt(t_data_no_id[log_col])  
        
        log_skew = skew(t_data_no_id[log_col].dropna())  
        sqrt_skew = skew(t_data_no_id[sqrt_col].dropna())

        skewness_comparison["Feature"].append(col)
        skewness_comparison["Log Skewness"].append(log_skew)
        skewness_comparison["Sqrt Skewness"].append(sqrt_skew)


skew_df = pd.DataFrame(skewness_comparison).set_index("Feature")

# Plot Comparison
skew_df.plot(kind="bar", figsize=(14, 6), colormap="coolwarm", width=0.8)
plt.axhline(y=0, color="black", linestyle="--", linewidth=1)
plt.title("Comparison of Skewness: Log Transform vs. Square Root Transform")
plt.ylabel("Skewness")
plt.xlabel("Features")
plt.legend(["Zero Skewness", "Log Transform", "Sqrt Transform"])
plt.xticks(rotation=90)
plt.show()








from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
feature_cols = [col for col in t_data_no_id.columns if col.startswith(("log_", "sqrt_"))]

X = t_data_no_id[feature_cols]
y = t_data_no_id["status_group"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
X_train = X_train[~np.isinf(X_train).any(axis=1)]
X_train = X_train[~np.isnan(X_train).any(axis=1)]

# Standardizing features for better model performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train logistic regression model
log_reg = LogisticRegression(solver="liblinear", penalty="l2")  
log_reg.fit(X_train_scaled, y_train)




print("X_train shape:", X_train.shape)









t_data_no_id = t_data.drop('id', axis=1)

t_data_no_id['date_recorded'] = pd.to_datetime(t_data_no_id['date_recorded'], format='%d/%m/%Y')
# Converting datetime to a numeric timestamp (in seconds)
t_data_no_id['date_recorded_numeric'] = t_data_no_id['date_recorded'].astype('int64') // 10**9

# Now compute the correlation matrix using the new numeric date column
numeric_data = t_data_no_id.select_dtypes(include=['number'])
corr = numeric_data.corr()
plt.figure(figsize=(15, 12))
sns.heatmap(corr, annot=True, fmt=".1f", cmap='coolwarm',annot_kws={"size": 8})
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()











t_data_no_id.describe





from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
numeric_df = t_data_no_id.select_dtypes(include=[np.number])
X = numeric_df.drop(columns=['status_group'], errors='ignore')
y = t_data_no_id['status_group']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

pipeline.fit(X_train, y_train)


test_values.describe


test_data = test_values.copy()
cat_imputer = SimpleImputer(strategy='most_frequent')
test_data[['permit', 'scheme_management', 'public_meeting', 
              'subvillage', 'funder', 'installer', 'wpt_name']] = \
    cat_imputer.fit_transform(test_data[['permit', 'scheme_management', 'public_meeting', 
                                             'subvillage', 'funder', 'installer', 'wpt_name']])
print(test_data.isna().sum())
print(test_data.head())


# Label encode ordinal columns
label_encode_cols = ['quality_group', 'quantity_group',  'water_quality', 'quantity']
le_dict = {}
for col in label_encode_cols:
    le = LabelEncoder()
    test_data[col] = le.fit_transform(test_data[col].astype(str))
    le_dict[col] = le

# One-hot encode nominal columns using pd.get_dummies
onehot_encode_cols = ['source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']
act_data = pd.get_dummies(test_data, columns=onehot_encode_cols, drop_first=True,  dtype=int)
print(act_data.head())


# since most of the features are categorical in nature we shall encode as follows
# Label encode ordinal columns
label_encode_cols = ['quality_group', 'quantity_group',  'water_quality', 'quantity']
le_dict = {}
for col in label_encode_cols:
    le = LabelEncoder()
    test_data[col] = le.fit_transform(test_data[col].astype(str))
    le_dict[col] = le

# One-hot encode nominal columns using pd.get_dummies
onehot_encode_cols = ['source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']
act_data = pd.get_dummies(test_data, columns=onehot_encode_cols, drop_first=True,  dtype=int)
print(act_data.head())


act_data_no_id = act_data.drop('id', axis=1)

act_data_no_id['date_recorded'] = pd.to_datetime(act_data_no_id['date_recorded'], format='%d/%m/%Y')
# Converting datetime to a numeric timestamp (in seconds)
act_data_no_id['date_recorded_numeric'] = act_data_no_id['date_recorded'].astype('int64') // 10**9

# Now compute the correlation matrix using the new numeric date column
numeric_data = act_data_no_id.select_dtypes(include=['number'])
corr = numeric_data.corr()
plt.figure(figsize=(15, 12))
sns.heatmap(corr, annot=True, fmt=".1f", cmap='coolwarm',annot_kws={"size": 8})
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()


numeric_d = act_data_no_id.select_dtypes(include=[np.number])
X_test = numeric_d
X_test.head





y_pred = pipeline.predict(X_test)  
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
plt.figure(figsize=(8, 6))
class_names = le.classes_
target_names = ['functional', 'non functional', 'functional needs repair']

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred, target_names=target_names))






